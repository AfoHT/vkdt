#version 460
#extension GL_GOOGLE_include_directive    : enable
#extension GL_EXT_nonuniform_qualifier    : enable

#include "shared.glsl"

layout(local_size_x = DT_LOCAL_SIZE_X, local_size_y = DT_LOCAL_SIZE_Y, local_size_z = 1) in;

layout(std140, set = 0, binding = 0) uniform global_t
{
  uint frame;
} global;

// global uniform stuff about image and roi
layout(std140, set = 0, binding = 1) uniform params_t
{
  float mode;
  float opacity;
} params;

layout( // input back buffer
    set = 1, binding = 0
) uniform sampler2D img_in;

layout( // new top layer
    set = 1, binding = 1
) uniform sampler2D img_top;

layout( // mask (single channel)
    set = 1, binding = 2
) uniform sampler2D img_mask;

layout( // output
    set = 1, binding = 3
) uniform writeonly image2D img_out;

void
main()
{
  ivec2 ipos = ivec2(gl_GlobalInvocationID);
  if(any(greaterThanEqual(ipos, imageSize(img_out)))) return;

  vec3 bck   = texelFetch(img_in,   ipos, 0).rgb;
  vec3 rgb   = texelFetch(img_top,  ipos, 0).rgb;
  // float mask = texelFetch(img_mask, ipos, 0).r;
  float mask = texture(img_mask, ipos/vec2(imageSize(img_out))).r;
  // TODO: switch based on mode, implement blend modes
  // if(global.frame > 0) rgb = mix(rgb, mix(rgb, bck, mask), params.opacity);
  // if(global.frame > 0) rgb = mix(rgb, bck, 1.0 - mask*params.opacity);
  // so now we need to talk about a noise model:
  // TODO: opacity as a sliding average, is this a good idea?
  // TODO: put noise sigma in here?
#if 1
  float t = clamp(params.opacity-mask, 0, 1);//clamp(params.opacity - m, 0, 1);
#if 0
  // this should only be a last resort thing to suppress rgb artifacts.
  const float sigma_noise = 0.3;
  float d = length(bck - rgb)/sigma_noise;
  t *= 1.-smoothstep(0.2, 3.0, d);
#endif

#if 1 // use TAA style box clamping
  vec3 mom1 = vec3(0.0f);
  vec3 mom2 = vec3(0.0f);
  const int r = 1;
  for(int yy = -r; yy <= r; yy++) {
    for(int xx = -r; xx <= r; xx++) {
      vec3 c = texelFetch(img_top, ipos + ivec2(xx, yy), 0).rgb;
      mom1 += c;
      mom2 += c * c;
    }
  }
  mom1 /= (2.0 * r + 1) * (2.0 * r + 1);
  mom2 /= (2.0 * r + 1) * (2.0 * r + 1);

  vec3 sigma = sqrt(max(vec3(0), mom2 - mom1 * mom1));
  const float thresh = 1.0;
  bck = clamp(bck, max(vec3(0), mom1 - thresh * sigma), mom1 + thresh * sigma);
#endif
#if 0  // sliding average / full accumulation is:
  // float N = global.frame + 1.0;
  // bck = (N-1.) / N * bck + rgb / N;
#endif
  if(global.frame > 0) rgb = mix(rgb, bck, t);
#endif



#if 0 // somehow the theory doesn't fit? mask as sqrt looks terrible indeed
  // bayesian risk for wavelet thresholding is minimised by
  // T = sigma^2 / sigma_noise   where sigma^2 = sigma_signal^2 + sigma_noise^2
  // mask is the L2 distance (of the whole patch) our image vs warped back buffer
  const float sigma_noise = 0.08; // i know that, it's been added synthetically
  const float T = 0.0001*params.opacity / sigma_noise;
  // const float d = length(bck - rgb);
  const float d = mask*mask;//sqrt(mask);
  // const float t = smoothstep(0.0, T, d);
  const float t = clamp((T-d)/T, 0.0, 1.0);
  if(global.frame > 0) rgb = mix(bck, rgb, t);
#endif

  imageStore(img_out, ipos, vec4(rgb, 1));
}

