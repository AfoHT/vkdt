#version 460
#extension GL_GOOGLE_include_directive    : enable
#extension GL_EXT_nonuniform_qualifier    : enable

#include "shared.glsl"

layout(local_size_x = DT_LOCAL_SIZE_X, local_size_y = DT_LOCAL_SIZE_Y, local_size_z = 1) in;

layout(std140, set = 0, binding = 1) uniform params_t
{
  float strength;
  float chroma;
  float luma;
  float envl;
  vec4  edge[9];
  vec4  blur[9];
  vec4  blck;
} params;

layout(push_constant, std140) uniform push_t
{
  vec4 wb;
  vec4 black;
  vec4 white;
  ivec4 crop;
  float noise_a;
  float noise_b;
  int level;
} push;

layout( // input
    set = 1, binding = 0
) uniform sampler2D img_in;

layout( // output
    set = 1, binding = 1
) uniform writeonly image2D img_out;

#if 1
// set of edge response filters
//     c2
//          c1
//       c
//   c3
//        c4
vec3 response(vec3 c0, vec3 c1, vec3 c2, vec3 c3, vec3 c4)
{
  // detect lines, corners, spots
  const float scale = push.level == 0 ? 1.0 : 65535.0*(push.white.g-push.black.g);
  const float sigma = 0.05 * pow(0.29, push.level) * sqrt(push.noise_a + scale*max(c0.x, 0.0)*push.noise_b);// /scale;
  const float dw[][5] = { // detection weights
    {params.edge[ 0].x, params.edge[ 0].y, params.edge[ 0].z, params.edge[ 0].w, params.edge[ 1].x},
    {params.edge[ 1].y, params.edge[ 1].z, params.edge[ 1].w, params.edge[ 2].x, params.edge[ 2].y},
    {params.edge[ 2].z, params.edge[ 2].w, params.edge[ 3].x, params.edge[ 3].y, params.edge[ 3].z},
    {params.edge[ 3].w, params.edge[ 4].x, params.edge[ 4].y, params.edge[ 4].z, params.edge[ 4].w},
    {params.edge[ 5].x, params.edge[ 5].y, params.edge[ 5].z, params.edge[ 5].w, params.edge[ 6].x},
    {params.edge[ 6].y, params.edge[ 6].z, params.edge[ 6].w, params.edge[ 7].x, params.edge[ 7].y},
    {params.edge[ 7].z, params.edge[ 7].w, params.edge[ 8].x, params.edge[ 8].y, params.edge[ 8].z}
  };

  const float t = 36.0/256.0; // preserve weight of center pixel, we're leaving away 8 samples in the boundary of the support.
  const float t2 = (1.0-t)/4.0;
  const float bw[][5] = { // blur weights
    {params.blur[ 0].x, params.blur[ 0].y, params.blur[ 0].z, params.blur[ 0].w, params.blur[ 1].x},
    {params.blur[ 1].y, params.blur[ 1].z, params.blur[ 1].w, params.blur[ 2].x, params.blur[ 2].y},
    {params.blur[ 2].z, params.blur[ 2].w, params.blur[ 3].x, params.blur[ 3].y, params.blur[ 3].z},
    {params.blur[ 3].w, params.blur[ 4].x, params.blur[ 4].y, params.blur[ 4].z, params.blur[ 4].w},
    {params.blur[ 5].x, params.blur[ 5].y, params.blur[ 5].z, params.blur[ 5].w, params.blur[ 6].x},
    {params.blur[ 6].y, params.blur[ 6].z, params.blur[ 6].w, params.blur[ 7].x, params.blur[ 7].y},
    {params.blur[ 7].z, params.blur[ 7].w, params.blur[ 8].x, params.blur[ 8].y, params.blur[ 8].z}
  };

  float maxres = -1e30;
  int idx = -1;

#if 1
  for(int i=0;i<7;i++)
  {
    // normalise edge weights to sum to 0:
    float c = (dw[i][0] + dw[i][1] + dw[i][2] + dw[i][3] + dw[i][4])/5.0;
    float res = dot((dw[i][0]-c) * c0 + (dw[i][1]-c) * c1 + (dw[i][2]-c) * c2 + (dw[i][3]-c) * c3 + (dw[i][4]-c) * c4, vec3(1,1,1));
    res = res*res; // ??? why tf do we need this?
    // TODO: normalise by mean brightness of pixels too? especially when comparing against the flat base case
    // TODO: filter 0 is the flat case, but it sums all values.. it should be scaled down somewhat!
    if(res > maxres)
    {
      maxres = res;
      idx = i;
    }
  }
#endif
  if(idx < 0) { idx = 0; maxres = 1.0; }
  // final filter: center envelope * (1/detection strength + edge weighted kernel) and normalise
  maxres = abs(maxres);
  float w[5] = {
    t  * (params.envl / maxres + bw[idx][0]),
    t2 * (params.envl / maxres + bw[idx][1]),
    t2 * (params.envl / maxres + bw[idx][2]),
    t2 * (params.envl / maxres + bw[idx][3]),
    t2 * (params.envl / maxres + bw[idx][4])};
  // return bw[nonuniformEXT(idx)][0] * c0 + bw[nonuniformEXT(idx)][1] * c1 + bw[nonuniformEXT(idx)][2] * c2 + bw[nonuniformEXT(idx)][3] * c3 + bw[nonuniformEXT(idx)][4] * c4;
  // return (bw[idx][0] * c0 + bw[idx][1] * c1 + bw[idx][2] * c2 + bw[idx][3] * c3 + bw[idx][4] * c4)/
    // (bw[idx][0] + bw[idx][1] + bw[idx][2] + bw[idx][3] + bw[idx][4]);
  return (w[0]*c0 + w[1]*c1 + w[2]*c2 + w[3]*c3 + w[4]*c4)/(w[0] + w[1] + w[2] + w[3] + w[4]);
}
#endif

void
main()
{
  ivec2 ipos = ivec2(gl_GlobalInvocationID);
  if(any(greaterThanEqual(ipos, imageSize(img_out)))) return;

  ivec2 sz = textureSize(img_in, 0).xy;

  // 5 tap flower filter
  const float t = 36.0/256.0; // preserve weight of center pixel, we're leaving away 8 samples in the boundary of the support.
#if 1 // feature detection wavelet
  vec3 c0 = texelFetch(img_in, ipos, 0).rgb;
  vec3 c1 = texture(img_in, (ipos + push.crop.xy + vec2(0.5 + 1.2, 0.5 + 0.4))/vec2(sz)).rgb;
  vec3 c2 = texture(img_in, (ipos + push.crop.xy + vec2(0.5 - 0.4, 0.5 + 1.2))/vec2(sz)).rgb;
  vec3 c3 = texture(img_in, (ipos + push.crop.xy + vec2(0.5 - 1.2, 0.5 - 0.4))/vec2(sz)).rgb;
  vec3 c4 = texture(img_in, (ipos + push.crop.xy + vec2(0.5 + 0.4, 0.5 - 1.2))/vec2(sz)).rgb;
  vec3 rgb = response(c0, c1, c2, c3, c4);
#else
#if 0 // regular wavelet
  vec3 rgb = texelFetch(img_in, ipos, 0).rgb * t;
  rgb += texture(img_in, (ipos + push.crop.xy + vec2(0.5 + 1.2, 0.5 + 0.4))/vec2(sz)).rgb / 4.0 * (1.0-t);
  rgb += texture(img_in, (ipos + push.crop.xy + vec2(0.5 - 1.2, 0.5 - 0.4))/vec2(sz)).rgb / 4.0 * (1.0-t);
  rgb += texture(img_in, (ipos + push.crop.xy + vec2(0.5 + 0.4, 0.5 - 1.2))/vec2(sz)).rgb / 4.0 * (1.0-t);
  rgb += texture(img_in, (ipos + push.crop.xy + vec2(0.5 - 0.4, 0.5 + 1.2))/vec2(sz)).rgb / 4.0 * (1.0-t);
#else // half way edge aware
  vec3 rgb = texelFetch(img_in, ipos, 0).rgb;
  vec3 sum = t * rgb;
  float wgt = t;
  // scale based on noise model and wavelet level:
  const float scale = push.level == 0 ? 1.0 : 65535.0*(push.white.g-push.black.g);
  const float sigma = pow(0.29, push.level) * sqrt(push.noise_a + scale*max(rgb.x, 0.0)*push.noise_b)/scale;
  // weight according to yuv or rgb:
  vec3 wc = 2./sigma *(push.level == 0 ? vec3(1.0) : vec3(1.0, 0.4, 0.4));

#define lookup(O) \
  do { \
  vec3 col = texture(img_in, (ipos + push.crop.xy + O)/vec2(sz)).rgb; \
  float w = exp(-dot(wc*(col-rgb),wc*(col-rgb))) * (1.0-t)/4.0; \
  sum += w * col; \
  wgt += w; \
  } while(false)

  lookup(vec2(0.5 + 1.2, 0.5 + 0.4));
  lookup(vec2(0.5 - 1.2, 0.5 - 0.4));
  lookup(vec2(0.5 + 0.4, 0.5 - 1.2));
  lookup(vec2(0.5 - 0.4, 0.5 + 1.2));

  rgb = sum / wgt;
#undef lookup

#endif
#endif

  // sort image to write positions so we can iterate the filter
  ivec2 off = ivec2(ipos.x & 1, ipos.y & 1) * sz/2;
  ipos = ipos / 2 + off;

  if(push.level == 0)
  {
    // this is for srgb not rec2020, but whatever
    const mat3 rgb_to_yuv = mat3(
        0.299, -0.147, 0.615,
        0.587, -0.289, -0.515,
        0.114, 0.436, -0.100);
    rgb = (rgb - push.black.rgb)/(push.white.rgb - push.black.rgb);
    rgb = rgb_to_yuv * (rgb * push.wb.rgb);
  }

  // TODO: consider 2px padding radius for next iteration and mirror repeat!
  imageStore(img_out, ipos, vec4(rgb, 1));
}

